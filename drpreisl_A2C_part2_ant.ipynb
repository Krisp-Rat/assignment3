{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:48:11.332448Z",
     "start_time": "2024-11-22T04:48:06.628640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from helper_functions import reward_print\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# AC2 algorithm \n",
    "class A2C:\n",
    "    def __init__(self, env):\n",
    "        self.actor = Actor(env)\n",
    "        self.critic = Critic(self.actor.env.observation_space.shape[0])\n",
    "    \n",
    "    # Main training loop\n",
    "    def train(self, episodes, gamma, greedy=False):\n",
    "        total_reward = [0] * episodes\n",
    "        for i in range(episodes):\n",
    "            step = rewards = 0\n",
    "            done = False     \n",
    "            state, info = self.actor.env.reset()\n",
    "            state = torch.tensor(state)\n",
    "            action_list = []\n",
    "            while not done:\n",
    "                # Actor makes decision \n",
    "                # Environment returns state and reward\n",
    "                next_state, reward, done, action = self.actor.act(state, greedy)\n",
    "                next_state = torch.tensor(next_state)#.todevice\n",
    "                # Critic evaluates action \n",
    "                adv = self.critic.evaluate(state, next_state, reward, gamma)\n",
    "                # Pass that value to the Actor\n",
    "                self.actor.evaluation(action, adv, state)\n",
    "                #action_list += action\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                rewards += reward\n",
    "                \n",
    "                # Before it is done\n",
    "                if done:\n",
    "                    reward = -10\n",
    "                    next_state = None\n",
    "                    adv = self.critic.evaluate(state, next_state, reward, gamma ** step)\n",
    "                    self.actor.evaluation(action, adv, state)\n",
    "                \n",
    "            total_reward[i] = rewards\n",
    "            print(\"Episode:\", i, \" Reward\", rewards)\n",
    "            #print(action_list)\n",
    "        self.actor.env.close()\n",
    "        return total_reward\n",
    "\n",
    "               \n",
    "    def save(self, filename):\n",
    "        with open(\"pickles/\" + filename + \"actor.pickle\", 'wb') as file:\n",
    "            pickle.dump(self.actor.policy_net.state_dict(), file)\n",
    "        with open(\"pickles/\" + filename + \"critic.pickle\", 'wb') as file:\n",
    "            pickle.dump(self.critic.policy_net.state_dict(), file)\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:48:11.340446Z",
     "start_time": "2024-11-22T04:48:11.333449Z"
    }
   },
   "id": "ce2167715cb125ae",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Actor thread\n",
    "class ActorNet(nn.Module):  \n",
    "    def __init__(self, obs, act):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.softmax(self.layer2(x), dim=-1)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, env):\n",
    "        self.env_type = env\n",
    "        self.env = gym.make(env)\n",
    "        self.policy_net1 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer1 = optim.AdamW(self.policy_net1.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net2 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer2 = optim.AdamW(self.policy_net2.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net3 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer3 = optim.AdamW(self.policy_net3.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net4 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer4 = optim.AdamW(self.policy_net4.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net5 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer5 = optim.AdamW(self.policy_net5.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net6 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer6 = optim.AdamW(self.policy_net6.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net7 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer7 = optim.AdamW(self.policy_net7.parameters(), amsgrad=True, lr=.001)\n",
    "        \n",
    "        self.policy_net8 = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer8 = optim.AdamW(self.policy_net8.parameters(), amsgrad=True, lr=.001)\n",
    "\n",
    "    \n",
    "    def act(self, state, greedy):\n",
    "        # Get the weights from the policy net\n",
    "        weights1 = self.policy_net1(state)\n",
    "        weights2 = self.policy_net2(state)\n",
    "        weights3 = self.policy_net3(state)\n",
    "        weights4 = self.policy_net4(state)\n",
    "        weights5 = self.policy_net5(state)\n",
    "        weights6 = self.policy_net6(state)\n",
    "        weights7 = self.policy_net7(state)\n",
    "        weights8 = self.policy_net8(state)\n",
    "        # if greedy get max-arg \n",
    "        if greedy: \n",
    "            action1 = torch.argmax(weights1)\n",
    "            action2 = torch.argmax(weights2)\n",
    "            action3 = torch.argmax(weights3)\n",
    "            action4 = torch.argmax(weights4)\n",
    "            action5 = torch.argmax(weights5)\n",
    "            action6 = torch.argmax(weights6)\n",
    "            action7 = torch.argmax(weights7)\n",
    "            action8 = torch.argmax(weights8)\n",
    "        # Use multinomial to select probability / action\n",
    "        else:\n",
    "            action1 = torch.multinomial(weights1, 1)\n",
    "            action2 = torch.multinomial(weights2, 1)\n",
    "            action3 = torch.multinomial(weights3, 1)\n",
    "            action4 = torch.multinomial(weights4, 1)\n",
    "            action5 = torch.multinomial(weights5, 1)\n",
    "            action6 = torch.multinomial(weights6, 1)\n",
    "            action7 = torch.multinomial(weights7, 1)\n",
    "            action8 = torch.multinomial(weights8, 1)\n",
    "\n",
    "        # Run and return the action \n",
    "        actions = [action1.item(), action2.item(), action3.item(), action4.item(), action5.item(), action6.item(), action7.item(), action8.item()]\n",
    "        state, reward, terminated, truncated, _ = self.env.step(actions)\n",
    "        return state, reward, terminated or truncated, actions\n",
    "\n",
    "    \n",
    "    def evaluation(self, action, advantage, state):\n",
    "        # Get the weights from the policy \n",
    "        weights1 = self.policy_net1(state)\n",
    "        weights2 = self.policy_net2(state)\n",
    "        weights3 = self.policy_net3(state)\n",
    "        weights4 = self.policy_net4(state)\n",
    "        weights5 = self.policy_net5(state)\n",
    "        weights6 = self.policy_net6(state)\n",
    "        weights7 = self.policy_net7(state)\n",
    "        weights8 = self.policy_net8(state)\n",
    "\n",
    "        # Calculate the log probability with the weights of the \n",
    "        # current state and action and then use the adv to get the loss \n",
    "        \n",
    "        prob1 = torch.distributions.Categorical(weights1).log_prob(torch.tensor(action[0]))  \n",
    "        loss1 = -1 * prob1 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        self.optimizer1.step()\n",
    "        \n",
    "        prob2 = torch.distributions.Categorical(weights2).log_prob(torch.tensor(action[1]))  \n",
    "        loss2 = -1 * prob2 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        self.optimizer2.step()\n",
    "        \n",
    "        prob3 = torch.distributions.Categorical(weights3).log_prob(torch.tensor(action[2]))  \n",
    "        loss3 = -1 * prob3 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer3.zero_grad()\n",
    "        loss3.backward()\n",
    "        self.optimizer3.step()\n",
    "        \n",
    "        prob4 = torch.distributions.Categorical(weights4).log_prob(torch.tensor(action[3]))  \n",
    "        loss4 = -1 * prob4 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer4.zero_grad()\n",
    "        loss4.backward()\n",
    "        self.optimizer4.step()\n",
    "        \n",
    "        prob5 = torch.distributions.Categorical(weights5).log_prob(torch.tensor(action[4]))  \n",
    "        loss5 = -1 * prob5 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer5.zero_grad()\n",
    "        loss5.backward()\n",
    "        self.optimizer5.step()\n",
    "        \n",
    "        prob6 = torch.distributions.Categorical(weights6).log_prob(torch.tensor(action[5]))  \n",
    "        loss6 = -1 * prob6 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer6.zero_grad()\n",
    "        loss6.backward()\n",
    "        self.optimizer6.step()\n",
    "        \n",
    "        prob7 = torch.distributions.Categorical(weights7).log_prob(torch.tensor(action[6]))  \n",
    "        loss7 = -1 * prob7 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer7.zero_grad()\n",
    "        loss7.backward()\n",
    "        self.optimizer7.step()\n",
    "        \n",
    "        prob8 = torch.distributions.Categorical(weights8).log_prob(torch.tensor(action[7]))  \n",
    "        loss8 = -1 * prob8 * advantage.detach()\n",
    "        # Backpropagation\n",
    "        self.optimizer8.zero_grad()\n",
    "        loss8.backward()\n",
    "        self.optimizer8.step()\n",
    "    \n",
    "    def change_render(self, render):\n",
    "        if render:\n",
    "            self.env = gym.make(self.env_type, render_mode=\"human\", max_episode_steps=200)\n",
    "        else: \n",
    "            self.env = gym.make(self.env_type, max_episode_steps=200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:53:18.237168Z",
     "start_time": "2024-11-22T04:53:18.209423Z"
    }
   },
   "id": "1afbc0d018df5ad3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Critic thread\n",
    "class CriticNet(nn.Module):  \n",
    "    def __init__(self, obs):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return  self.layer2(x)\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, obs):\n",
    "        \n",
    "        self.policy_net = CriticNet(obs)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True, lr=.001)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, state, next_state, reward, gamma):\n",
    "        # Get Qvalue and next Qvalue from policy         \n",
    "        Qvalue = self.policy_net(state)\n",
    "        if next_state is not None:\n",
    "            next_Qvalue = self.policy_net(next_state)\n",
    "        else: \n",
    "            next_Qvalue = 0\n",
    "        \n",
    "        # Calculate the TD and advantage for the next action\n",
    "        TD = reward + (gamma * next_Qvalue)\n",
    "        # print(next_Qvalue)\n",
    "        adv = TD - Qvalue\n",
    "        TD = torch.tensor([TD])\n",
    "        loss_function = nn.MSELoss()\n",
    "        loss = loss_function(Qvalue, TD)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return adv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:53:21.212387Z",
     "start_time": "2024-11-22T04:53:21.207348Z"
    }
   },
   "id": "463d798db189e3a9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m agent\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39mchange_render(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Main training session\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mmax\u001B[39m(total_rewards))\n\u001B[0;32m     12\u001B[0m reward_print(total_rewards, episodes, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[2], line 19\u001B[0m, in \u001B[0;36mA2C.train\u001B[1;34m(self, episodes, gamma, greedy)\u001B[0m\n\u001B[0;32m     15\u001B[0m action_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m# Actor makes decision \u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Environment returns state and reward\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m     next_state, reward, done, action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgreedy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(next_state)\u001B[38;5;66;03m#.todevice\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Critic evaluates action \u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 45\u001B[0m, in \u001B[0;36mActor.act\u001B[1;34m(self, state, greedy)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mact\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, greedy):\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;66;03m# Get the weights from the policy net\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m     weights1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_net1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m     weights2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net2(state)\n\u001B[0;32m     47\u001B[0m     weights3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net3(state)\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 9\u001B[0m, in \u001B[0;36mActorNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m----> 9\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     10\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "# AC2 Agent for Cart Pole\n",
    "environment = \"Ant-v5\"\n",
    "agent = A2C(environment)\n",
    "\n",
    "episodes = 500\n",
    "gamma = .99\n",
    "\n",
    "agent.actor.change_render(True)\n",
    "# Main training session\n",
    "total_rewards = agent.train(episodes, gamma)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "reward_print(total_rewards, episodes, \"Ant\")\n",
    "\n",
    "# Greedy run \n",
    "agent.actor.change_render(True)\n",
    "total_greedy_rewards = agent.train(11, gamma, greedy=True)\n",
    "reward_print(total_greedy_rewards, 10, \"greedy Ant\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:53:25.779778Z",
     "start_time": "2024-11-22T04:53:22.826536Z"
    }
   },
   "id": "8ba35e36a764eaec",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prints the final graph and saves final weights\n",
    "\n",
    "print(\"Average\", sum(total_rewards)/ len(total_rewards))\n",
    "reward_print(total_rewards, episodes, \"Ant\")\n",
    "reward_print(total_greedy_rewards, 10, \"Greedy Ant\")\n",
    "agent.save(\"drpreisl_BipedalWalker\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a10cd02206f84"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
