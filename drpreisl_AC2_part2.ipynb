{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:23:27.105563Z",
     "start_time": "2024-11-22T04:23:27.101373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from helper_functions import reward_print\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# AC2 algorithm \n",
    "class A2C:\n",
    "    def __init__(self, env):\n",
    "        self.actor = Actor(env)\n",
    "        self.critic = Critic(self.actor.env.observation_space.shape[0])\n",
    "    \n",
    "    # Main training loop\n",
    "    def train(self, episodes, gamma, greedy=False):\n",
    "        total_reward = [0] * episodes\n",
    "        for i in range(episodes):\n",
    "            step = rewards = 0\n",
    "            done = False     \n",
    "            state, info = self.actor.env.reset()\n",
    "            state = torch.tensor(state)\n",
    "            action_list = []\n",
    "            while not done:\n",
    "                # Actor makes decision \n",
    "                # Environment returns state and reward\n",
    "                next_state, reward, done, action = self.actor.act(state, greedy)\n",
    "                next_state = torch.tensor(next_state)#.todevice\n",
    "                # Critic evaluates action \n",
    "                adv = self.critic.evaluate(state, next_state, reward, gamma)\n",
    "                # Pass that value to the Actor\n",
    "                self.actor.evaluation(action, adv, state)\n",
    "                #action_list += action\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                rewards += reward\n",
    "                \n",
    "                # Before it is done\n",
    "                if done:\n",
    "                    reward = -10\n",
    "                    next_state = None\n",
    "                    adv = self.critic.evaluate(state, next_state, reward, gamma ** step)\n",
    "                    self.actor.evaluation(action, adv, state)\n",
    "                \n",
    "            total_reward[i] = rewards\n",
    "            print(\"Episode:\", i, \" Reward\", rewards)\n",
    "            #print(action_list)\n",
    "        self.actor.env.close()\n",
    "        return total_reward\n",
    "\n",
    "               \n",
    "    def save(self, filename):\n",
    "        with open(\"pickles/\" + filename + \"actor.pickle\", 'wb') as file:\n",
    "            pickle.dump(self.actor.policy_net.state_dict(), file)\n",
    "        with open(\"pickles/\" + filename + \"critic.pickle\", 'wb') as file:\n",
    "            pickle.dump(self.critic.policy_net.state_dict(), file)\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:23:28.825663Z",
     "start_time": "2024-11-22T04:23:28.819982Z"
    }
   },
   "id": "695f77b6e4bea45c",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Actor thread\n",
    "class ActorNet(nn.Module):  \n",
    "    def __init__(self, obs, act):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.softmax(self.layer2(x), dim=-1)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, env):\n",
    "        self.env_type = env\n",
    "        self.env = gym.make(env)\n",
    "        self.policy_net = ActorNet(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), amsgrad=True, lr=.001)\n",
    "    \n",
    "    def act(self, state, greedy):\n",
    "        # Get the weights from the policy net\n",
    "        weights = self.policy_net(state)\n",
    "        # if greedy get max-arg \n",
    "        if greedy: \n",
    "            action = torch.argmax(weights)\n",
    "        # Use multinomial to select probability / action\n",
    "        else:\n",
    "            action = torch.multinomial(weights, 1)\n",
    "        # Run and return the action \n",
    "        state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "        return state, reward, terminated or truncated, action\n",
    "\n",
    "    \n",
    "    def evaluation(self, action, advantage, state):\n",
    "        # Get the weights from the policy \n",
    "        weight = self.policy_net(state)\n",
    "        # Calculate the log probability with the weights of the \n",
    "        # current state and action and then use the adv to get the loss \n",
    "        prob = torch.distributions.Categorical(weight).log_prob(action)  \n",
    "        loss = -1 * prob * advantage.detach()\n",
    "        # back prop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    \n",
    "    def change_render(self, render):\n",
    "        if render:\n",
    "            self.env = gym.make(self.env_type, render_mode=\"human\", max_episode_steps=200)\n",
    "        else: \n",
    "            self.env = gym.make(self.env_type, max_episode_steps=200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:23:30.704627Z",
     "start_time": "2024-11-22T04:23:30.698035Z"
    }
   },
   "id": "94317ee46691fa0b",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Critic thread\n",
    "class CriticNet(nn.Module):  \n",
    "    def __init__(self, obs):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return  self.layer2(x)\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, obs):\n",
    "        \n",
    "        self.policy_net = CriticNet(obs)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True, lr=.001)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, state, next_state, reward, gamma):\n",
    "        # Get Qvalue and next Qvalue from policy         \n",
    "        Qvalue = self.policy_net(state)\n",
    "        if next_state is not None:\n",
    "            next_Qvalue = self.policy_net(next_state)\n",
    "        else: \n",
    "            next_Qvalue = 0\n",
    "        \n",
    "        # Calculate the TD and advantage for the next action\n",
    "        TD = reward + (gamma * next_Qvalue)\n",
    "        # print(next_Qvalue)\n",
    "        adv = TD - Qvalue\n",
    "        TD = torch.tensor([TD])\n",
    "        loss_function = nn.MSELoss()\n",
    "        loss = loss_function(Qvalue, TD)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return adv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:23:32.388127Z",
     "start_time": "2024-11-22T04:23:32.382551Z"
    }
   },
   "id": "1abb4bcf7996ff56",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m agent\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39mchange_render(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Main training session\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mmax\u001B[39m(total_rewards))\n\u001B[0;32m     12\u001B[0m reward_print(total_rewards, episodes, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrid world\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[27], line 19\u001B[0m, in \u001B[0;36mA2C.train\u001B[1;34m(self, episodes, gamma, greedy)\u001B[0m\n\u001B[0;32m     15\u001B[0m action_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m# Actor makes decision \u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Environment returns state and reward\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m     next_state, reward, done, action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgreedy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(next_state)\u001B[38;5;66;03m#.todevice\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Critic evaluates action \u001B[39;00m\n",
      "Cell \u001B[1;32mIn[28], line 31\u001B[0m, in \u001B[0;36mActor.act\u001B[1;34m(self, state, greedy)\u001B[0m\n\u001B[0;32m     29\u001B[0m     action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmultinomial(weights, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Run and return the action \u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m state, reward, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m state, reward, terminated \u001B[38;5;129;01mor\u001B[39;00m truncated, action\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[0;32m    114\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    115\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    123\u001B[0m \n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 125\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\core.py:322\u001B[0m, in \u001B[0;36mWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[0;32m    320\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    321\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:283\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_step_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:207\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[1;34m(env, action)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001B[39;00m\n\u001B[1;32m--> 207\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m    209\u001B[0m     result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[0;32m    210\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects step result to be a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:535\u001B[0m, in \u001B[0;36mBipedalWalker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoints[\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39mmotorSpeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(SPEED_KNEE \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(action[\u001B[38;5;241m3\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    534\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 535\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoints[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmotorSpeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(SPEED_HIP \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msign(\u001B[43maction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m))\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoints[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmaxMotorTorque \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\n\u001B[0;32m    537\u001B[0m         MOTORS_TORQUE \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(np\u001B[38;5;241m.\u001B[39mabs(action[\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    538\u001B[0m     )\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoints[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mmotorSpeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(SPEED_KNEE \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msign(action[\u001B[38;5;241m1\u001B[39m]))\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# AC2 Agent for Cart Pole\n",
    "environment = 'BipedalWalker-v3'\n",
    "agent = A2C(environment)\n",
    "\n",
    "episodes = 10\n",
    "gamma = .99\n",
    "\n",
    "agent.actor.change_render(True)\n",
    "# Main training session\n",
    "total_rewards = agent.train(episodes, gamma)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "\n",
    "# Greedy run \n",
    "agent.actor.change_render(True)\n",
    "total_greedy_rewards = agent.train(11, gamma, greedy=True)\n",
    "reward_print(total_greedy_rewards, 10, \"greedy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:23:35.611090Z",
     "start_time": "2024-11-22T04:23:34.230076Z"
    }
   },
   "id": "bce748e51903f4ac",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "-0.21270431207492949\n",
      "-0.13174029403676474\n",
      "-0.13460298477610189\n",
      "-0.07263028202826542\n",
      "-0.11699600247542181\n",
      "-0.12238068797190867\n",
      "-0.13231642260154325\n",
      "-0.12885902671018998\n",
      "-0.11081495374441147\n",
      "-0.10771559218565743\n",
      "-0.10196031949917593\n",
      "-0.1442953138748781\n",
      "-0.1327311413884163\n",
      "-0.06783421353499214\n",
      "-0.12090195897221565\n",
      "-0.1344563980102539\n",
      "-0.1935158486366272\n",
      "-0.2191252047518901\n",
      "-0.21145857246716698\n",
      "-0.20636485616366188\n",
      "-0.22125689905881885\n",
      "-0.10998067883650224\n",
      "-0.10998010283708573\n",
      "-0.03342592541376868\n",
      "-0.001167571504909603\n",
      "-0.025699895352125166\n",
      "-0.13610690450171747\n",
      "-0.17660249789555746\n",
      "-0.22833079299579184\n",
      "-0.256558383256197\n",
      "-0.19351435136795045\n",
      "-0.04736393974721432\n",
      "-0.0723199452161789\n",
      "-0.02540315878391266\n",
      "0.012657308687765212\n",
      "0.04901599508523942\n",
      "0.11854570941627024\n",
      "0.015809107462566287\n",
      "0.06425259360174457\n",
      "-0.04718124775091926\n",
      "-0.05876371077696601\n",
      "-0.06533790675798816\n",
      "-0.04977824911133934\n",
      "-0.0994478390614168\n",
      "-0.16975913031896195\n",
      "-0.12072812155137222\n",
      "-0.12963821458568414\n",
      "-0.14022289518515035\n",
      "-0.07256416749954223\n",
      "-0.1653538664082698\n",
      "-0.09164118933677673\n",
      "-0.18061707536379615\n",
      "-0.09883118291695794\n",
      "-0.0390220032532998\n",
      "-0.05218011226256925\n",
      "-0.02144562085469446\n",
      "-0.057797963351011276\n",
      "-0.04859371431668481\n",
      "-0.05328870159387588\n",
      "-0.024451956232391187\n",
      "2.9238412778766036e-05\n",
      "-0.16232331458727636\n",
      "0.0030689620077610016\n",
      "-0.04091777968406678\n",
      "-0.0679734581708908\n",
      "-0.12497183652718744\n",
      "-0.08745507406753679\n",
      "-0.17869169163703919\n",
      "-0.12844047387441235\n",
      "0.005898012300334755\n",
      "0.03079993303616723\n",
      "-0.010655898948509303\n",
      "-0.07438273350397864\n",
      "-0.015021354873973937\n",
      "0.017855428119499292\n",
      "0.05556034777561941\n",
      "0.0755630995929241\n",
      "0.1254951277375221\n",
      "0.14585390077034394\n",
      "0.02240801084041595\n",
      "0.02382938572764397\n",
      "0.0461643112897873\n",
      "0.07184840170542517\n",
      "0.15184379764397818\n",
      "0.14214657109975817\n",
      "0.1075361582438163\n",
      "0.10943907417853556\n",
      "0.1255871477325781\n",
      "0.046336681922275635\n",
      "0.08636486051480093\n",
      "0.04360047026475153\n",
      "-0.032224166949588864\n",
      "-0.09941523516178132\n",
      "-0.012995213354628478\n",
      "-0.0517425630489997\n",
      "-0.0644980407655239\n",
      "-0.031318327983219235\n",
      "-0.08325480395555496\n",
      "-0.053434474984805976\n",
      "-0.07330200894673547\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make('BipedalWalker-v3', render_mode=\"human\")\n",
    "\n",
    "state, info = environment.reset()\n",
    "print(environment.action_space.shape[0])\n",
    "done = False\n",
    "for i in range(100):\n",
    "    if not done:\n",
    "        state, reward, terminated, truncated, _  = environment.step(environment.action_space.sample())\n",
    "        done = terminated or truncated\n",
    "        print(reward)\n",
    "    else:\n",
    "        break\n",
    "environment.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T04:05:04.023187Z",
     "start_time": "2024-11-22T04:05:01.857750Z"
    }
   },
   "id": "6e3e89d3262630d4",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add episodes to avoid having to retrain 1000+ episodes\n",
    "add_episodes = 400\n",
    "agent.actor.change_render(True)\n",
    "\n",
    "# Main training session\n",
    "add_total_rewards = agent.train(add_episodes, gamma)\n",
    "total_rewards += add_total_rewards\n",
    "episodes += add_episodes\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "reward_print(total_rewards, episodes, \"Cart Pole\")\n",
    "\n",
    "# Greedy run \n",
    "agent.actor.change_render(True)\n",
    "total_greedy_rewards = agent.train(11, gamma, greedy=True)\n",
    "reward_print(total_greedy_rewards, 10, \"greedy\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c115a1203c43917a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Prints the final graph and saves final weights\n",
    "\n",
    "print(\"Average\", sum(total_rewards)/ len(total_rewards))\n",
    "reward_print(total_rewards, episodes, \"Bipedal Walker\")\n",
    "reward_print(total_greedy_rewards, 10, \"Bipedal Walker\")\n",
    "agent.save(\"drpreisl_BipedalWalker\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T20:10:53.941197Z",
     "start_time": "2024-11-21T20:10:53.937677Z"
    }
   },
   "id": "be4186926fc64110",
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
