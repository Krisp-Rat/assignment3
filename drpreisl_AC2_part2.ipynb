{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T00:33:46.621399Z",
     "start_time": "2024-11-22T00:33:44.907667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from helper_functions import reward_print, print_Qtable\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# AC2 algorithm \n",
    "class AC2:\n",
    "    def __init__(self, env):\n",
    "        self.actor = Actor(env)\n",
    "        self.critic = Critic(self.actor.env.observation_space.shape[0])\n",
    "    \n",
    "    # Main training loop\n",
    "    def train(self, episodes, gamma, greedy=False):\n",
    "        total_reward = [0] * episodes\n",
    "        for i in range(episodes):\n",
    "            step = rewards = 0\n",
    "            done = False     \n",
    "            state, info = self.actor.env.reset()\n",
    "            state = torch.tensor(state)\n",
    "            while not done:\n",
    "                # Actor makes decision \n",
    "                # Environment returns state and reward\n",
    "                next_state, reward, done, action = self.actor.act(state, greedy)\n",
    "                next_state = torch.tensor(next_state)#.todevice\n",
    "                # Critic evaluates action \n",
    "                adv = self.critic.evaluate(state, next_state, reward, gamma ** step)\n",
    "                # Pass that value to the Actor\n",
    "                self.actor.evaluation(action, adv, state)\n",
    "                \n",
    "                state = next_state\n",
    "                step += 1\n",
    "                rewards += reward\n",
    "                \n",
    "                # Before it is done\n",
    "                if done:\n",
    "                    reward = 0\n",
    "                    next_state = None\n",
    "                    adv = self.critic.evaluate(state, next_state, reward, gamma ** step)\n",
    "                    self.actor.evaluation(action, adv, state)\n",
    "                \n",
    "            total_reward[i] = rewards\n",
    "            print(\"Episode:\", i, \" Reward\", rewards)\n",
    "        self.actor.env.close()\n",
    "        return total_reward\n",
    "\n",
    "               \n",
    "    def save(self, filename):\n",
    "        with open(\"pickles/\" + filename + \"actor.pickle\", 'wb') as file:\n",
    "            pickle.dump(self.actor.policy_net.state_dict(), file)\n",
    "        with open(\"pickles/\" + filename + \"critic.pickle\", 'wb') as file:\n",
    "            pickle.dump(self.critic.policy_net.state_dict(), file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T00:33:48.739984Z",
     "start_time": "2024-11-22T00:33:48.733984Z"
    }
   },
   "id": "695f77b6e4bea45c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Actor thread\n",
    "class ActorNet(nn.Module):  \n",
    "    def __init__(self, obs, act):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.softmax(self.layer2(x), dim=-1)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, env):\n",
    "        self.env_type = env\n",
    "        self.env = gym.make(env)\n",
    "        self.policy_net = ActorNet(self.env.observation_space.shape[0], self.env.action_space.n)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), amsgrad=True)\n",
    "    \n",
    "    def act(self, state, greedy):\n",
    "        # Get the weights from the policy net\n",
    "        weights = self.policy_net(state)\n",
    "        # if greedy get max-arg \n",
    "        if greedy: \n",
    "            action = torch.argmax(weights)\n",
    "        # Use multinomial to select probability / action\n",
    "        else:\n",
    "            action = torch.multinomial(weights, 1)\n",
    "        # Run and return the action \n",
    "        state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "        return state, reward, terminated or truncated, action\n",
    "\n",
    "    \n",
    "    def evaluation(self, action, advantage, state):\n",
    "        # Get the weights from the policy \n",
    "        weight = self.policy_net(state)\n",
    "        # Calculate the log probability with the weights of the \n",
    "        # current state and action and then use the adv to get the loss \n",
    "        prob = torch.distributions.Categorical(weight).log_prob(action)  \n",
    "        loss = -1 * prob * advantage.detach()\n",
    "        # back prop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    \n",
    "    def change_render(self, render):\n",
    "        if render:\n",
    "            self.env = gym.make(self.env_type, render_mode=\"human\")\n",
    "        else: \n",
    "            self.env = gym.make(self.env_type)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T00:33:51.436364Z",
     "start_time": "2024-11-22T00:33:51.429777Z"
    }
   },
   "id": "94317ee46691fa0b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Critic thread\n",
    "class CriticNet(nn.Module):  \n",
    "    def __init__(self, obs):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return  self.layer2(x)\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, obs):\n",
    "        \n",
    "        self.policy_net = CriticNet(obs)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True )\n",
    "    \n",
    "    \n",
    "    def evaluate(self, state, next_state, reward, gamma):\n",
    "        # Get Qvalue and next Qvalue from policy         \n",
    "        Qvalue = self.policy_net(state)\n",
    "        if next_state is not None:\n",
    "            next_Qvalue = self.policy_net(next_state)\n",
    "        else: \n",
    "            next_Qvalue = 0\n",
    "        \n",
    "        # Calculate the TD and advantage for the next action\n",
    "        TD = reward + (gamma * next_Qvalue)\n",
    "        adv = Qvalue - TD\n",
    "        TD = torch.tensor([TD])\n",
    "        loss_function = nn.MSELoss()\n",
    "        # print(\"Q:\", Qvalue)\n",
    "        # print(\"TD:\", TD)\n",
    "        # print(\"adv:\", adv)\n",
    "        loss = loss_function(Qvalue, TD)\n",
    "        # print(loss)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return adv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T00:33:54.162922Z",
     "start_time": "2024-11-22T00:33:54.156900Z"
    }
   },
   "id": "1abb4bcf7996ff56",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# AC2 Agent for Cart Pole\u001B[39;00m\n\u001B[0;32m      2\u001B[0m environment \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBipedalWalker-v3\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m agent \u001B[38;5;241m=\u001B[39m \u001B[43mAC2\u001B[49m\u001B[43m(\u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m episodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      6\u001B[0m gamma \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.003\u001B[39m\n",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m, in \u001B[0;36mAC2.__init__\u001B[1;34m(self, env)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, env):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor \u001B[38;5;241m=\u001B[39m \u001B[43mActor\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcritic \u001B[38;5;241m=\u001B[39m Critic(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn[3], line 18\u001B[0m, in \u001B[0;36mActor.__init__\u001B[1;34m(self, env)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_type \u001B[38;5;241m=\u001B[39m env\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(env)\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net \u001B[38;5;241m=\u001B[39m ActorNet(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn\u001B[49m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdamW(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net\u001B[38;5;241m.\u001B[39mparameters(), amsgrad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# AC2 Agent for Cart Pole\n",
    "environment = 'BipedalWalker-v3'\n",
    "agent = AC2(environment)\n",
    "\n",
    "episodes = 10\n",
    "gamma = 1.003\n",
    "\n",
    "agent.actor.change_render(True)\n",
    "\n",
    "# Main training session\n",
    "total_rewards = agent.train(episodes, gamma)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "agent.save(\"drpreisl_CartPole\")\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "\n",
    "# Greedy run \n",
    "agent.actor.change_render(True)\n",
    "total_greedy_rewards = agent.train(11, gamma, greedy=True)\n",
    "reward_print(total_greedy_rewards, 10, \"greedy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T00:34:45.628840Z",
     "start_time": "2024-11-22T00:34:45.413321Z"
    }
   },
   "id": "bce748e51903f4ac",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "state, info = environment.reset()\n",
    "\n",
    "done = False\n",
    "for i in range(100):\n",
    "    if not done:\n",
    "        state, reward, terminated, truncated, _  = environment.step(environment.action_space.sample())\n",
    "        done = terminated or truncated\n",
    "        # print(reward)\n",
    "    else:\n",
    "        break\n",
    "environment.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T07:56:23.267595Z",
     "start_time": "2024-11-21T07:56:21.897770Z"
    }
   },
   "id": "6e3e89d3262630d4",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "TD = 1234\n",
    "TD = torch.tensor([4, 3, 6, 1])\n",
    "print(torch.argmax(TD).item())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T20:10:53.941197Z",
     "start_time": "2024-11-21T20:10:53.937677Z"
    }
   },
   "id": "be4186926fc64110",
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
