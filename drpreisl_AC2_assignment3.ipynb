{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-20T04:00:43.935563Z",
     "start_time": "2024-11-20T04:00:42.043235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from helper_functions import reward_print, print_Qtable\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T02:49:20.147257Z",
     "start_time": "2024-11-20T02:49:20.142671Z"
    }
   },
   "id": "f37aefa4ef1ac047",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# AC2 algorithm \n",
    "class AC2:\n",
    "    def __init__(self, env):\n",
    "        self.actor = Actor(env)\n",
    "        self.critic = Critic(env)\n",
    "    \n",
    "    # Main training loop\n",
    "    def train(self, episodes, gamma, greedy=False):\n",
    "        total_reward = [0] * episodes\n",
    "        for i in range(episodes):\n",
    "            step = rewards = 0\n",
    "            done = False            \n",
    "            while not done:\n",
    "                # Actor makes decision \n",
    "                action = self.actor.act(self, state)\n",
    "                # Environment returns state and reward\n",
    "                next_state, reward, terminated, truncated, _ = self.actor.step(action)\n",
    "                done = terminated or truncated \n",
    "                # Critic evaluates action \n",
    "                value = self.critic.evaluate(next_state, reward)\n",
    "                # Pass that value to the Actor\n",
    "                self.actor.evaluation(value)\n",
    "                \n",
    "                step += 1\n",
    "                rewards += reward\n",
    "                \n",
    "            total_reward[i] = rewards      \n",
    "        return total_reward\n",
    "\n",
    "               \n",
    "    def save(self, filename):\n",
    "        placeholder = ''\n",
    "        with open(\"pickles/\" + filename, 'wb') as file:\n",
    "            pickle.dump(placeholder, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T02:42:46.139074Z",
     "start_time": "2024-11-20T02:42:46.133846Z"
    }
   },
   "id": "695f77b6e4bea45c",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Actor thread\n",
    "class Actor:\n",
    "    def __init__(self, env):\n",
    "        self.env_type = env\n",
    "        self.env = gym.make(env)\n",
    "        state, info = self.env.reset()\n",
    "        self.policy_net = Net(len(state), self.env.action_space.n)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), amsgrad=True)\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        action = self.pick_action(state)\n",
    "        state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        return state, reward, terminated, truncated, _ \n",
    "        \n",
    "        \n",
    "    def pick_action(self, state):\n",
    "        # Get probability list from policy net\n",
    "        weights = self.policy_net(state)\n",
    "        # Apply that list to the action list to get the appropriate action \n",
    "        action = torch.multinomial(weights, 1)#.todevice\n",
    "        return action.item()\n",
    "    \n",
    "    \n",
    "    def evaluation(self, values):\n",
    "        # Need to update the actors policy with the critics evaluation \n",
    "        # use softmaxing...\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def change_render(self, render):\n",
    "        if render:\n",
    "            self.env = gym.make(self.env_type, render=\"human\")\n",
    "        else: \n",
    "            self.env = gym.make(self.env_type)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T02:24:57.593925Z",
     "start_time": "2024-11-20T02:24:57.590214Z"
    }
   },
   "id": "94317ee46691fa0b",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Critic thread\n",
    "class Critic:\n",
    "    def __init__(self, obs, action):\n",
    "        \n",
    "        self.policy_net = Net(obs, action)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True )\n",
    "    \n",
    "    \n",
    "    def evaluate(self, state, next_state, reward):\n",
    "        # Need to generate an evaluation to update policy \n",
    "        # Calculate the Q value\n",
    "        # Calculate the value function\n",
    "        # Soft-maxing ???\n",
    "        torch.gradient()\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T02:24:58.945866Z",
     "start_time": "2024-11-20T02:24:58.942855Z"
    }
   },
   "id": "1abb4bcf7996ff56",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# AC2 Agent for Cart Pole\n",
    "environment = 'CartPole-v1'\n",
    "agent = AC2(environment)\n",
    "\n",
    "episodes = 10\n",
    "gamma = 1.003\n",
    "\n",
    "agent.actor.change_render(True)\n",
    "\n",
    "# Main training session\n",
    "total_rewards = agent.train(episodes, gamma)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "agent.save(\"drpreisl_part1_assignment3.pickle\")\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "\n",
    "# Greedy run \n",
    "agent.actor.change_render(True)\n",
    "total_greedy_rewards = agent.train(11, gamma, greedy=True)\n",
    "reward_print(total_greedy_rewards, 10, \"greedy\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce748e51903f4ac"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39208955 -0.49994665 -0.8134083  -0.8935776 ]\n",
      "[ 0.996924   -0.46270904  0.25332296 -0.6199383 ]\n",
      "[-0.16606212  0.8293581  -0.0554404  -0.9682666 ]\n",
      "[ 0.13676018  0.23906039 -0.56972533  0.92407334]\n",
      "[ 0.75992817 -0.8317102  -0.05071194  0.20298636]\n",
      "[-0.49637958 -0.20431677 -0.7113743  -0.91306967]\n",
      "[ 0.43331325 -0.9967853  -0.21113028 -0.84131455]\n",
      "[-0.653226    0.1399365   0.86226684 -0.6065078 ]\n",
      "[-0.22277948  0.24012399  0.8187429   0.25846824]\n",
      "[-0.7382522   0.82068515 -0.20854723 -0.5146232 ]\n",
      "[-0.60003424  0.5194155  -0.44214278 -0.6722367 ]\n",
      "[-0.94243145  0.7762538   0.21926855  0.17618997]\n",
      "[ 0.3296068  -0.7945417  -0.68480724 -0.5729555 ]\n",
      "[-0.20722602 -0.2032666  -0.40193555 -0.9935767 ]\n",
      "[ 0.8102185  -0.99676305  0.41692215  0.7451281 ]\n",
      "[-0.66372377 -0.8134068   0.23761183 -0.10493127]\n",
      "[-0.1900641  -0.29664922  0.47275737 -0.6611509 ]\n",
      "[-0.395759   -0.5186109  -0.06502593 -0.2105364 ]\n",
      "[-0.15255351  0.23339204 -0.05762069  0.8820962 ]\n",
      "[-0.4325765  -0.62420124  0.00941765 -0.729864  ]\n",
      "[-0.5786223  -0.48743406 -0.967093    0.5789208 ]\n",
      "[-0.07123093 -0.04844444 -0.33932957 -0.801893  ]\n",
      "[-0.9654104   0.2599526   0.24008156  0.03082764]\n",
      "[-0.32160157  0.8634465  -0.6279373   0.35136643]\n",
      "[-0.30964014 -0.4184333   0.6424645  -0.5168823 ]\n",
      "[-0.781464    0.7924174  -0.11561054 -0.17418095]\n",
      "[0.7167706  0.57341504 0.9581722  0.9754651 ]\n",
      "[ 0.99557596 -0.07240585 -0.6448242   0.5516898 ]\n",
      "[-0.8633967  -0.898983    0.68683034  0.8576856 ]\n",
      "[ 0.8758426   0.7507094  -0.44732094  0.40662178]\n",
      "[ 0.55279243 -0.47894683  0.0072912   0.04893393]\n",
      "[-0.45262837 -0.8147363  -0.80548733 -0.9477519 ]\n",
      "[-0.9374843 -0.3078953 -0.4455807  0.5699618]\n",
      "[-0.4255398   0.00859334 -0.11930822 -0.4257633 ]\n",
      "[ 0.5005057  -0.81793207  0.7795191   0.08088116]\n",
      "[-0.2511587   0.100111    0.27679598  0.02581853]\n",
      "[ 0.7256055  -0.7424239  -0.33758548 -0.1437237 ]\n",
      "[ 0.08671765  0.38427863 -0.05731177 -0.25926504]\n",
      "[ 0.70437413  0.21224222 -0.7563521   0.82019603]\n",
      "[-0.6654023   0.04085906 -0.45539758  0.05359817]\n",
      "[-0.92767906  0.93472624  0.77122146 -0.53978246]\n",
      "[-0.14864852  0.46477935  0.90037996  0.8905279 ]\n",
      "[ 0.5769447  -0.95348316  0.5144605   0.3729386 ]\n",
      "[-0.23399994  0.384419    0.859756    0.8804136 ]\n",
      "[-0.30328012  0.49942774 -0.05868612 -0.2827708 ]\n",
      "[0.17517182 0.49674848 0.16805185 0.42295894]\n",
      "[-0.85342735 -0.7921551   0.23747438 -0.54484606]\n",
      "[0.13664904 0.74869615 0.05074332 0.12592952]\n",
      "[ 0.48679778 -0.28455037 -0.2723104   0.6996998 ]\n",
      "[-0.16184431 -0.37882602 -0.5025628   0.11096713]\n",
      "[-0.31094864 -0.56899655  0.30948055  0.3772905 ]\n",
      "[ 0.38307598 -0.51277304  0.7992916   0.13174061]\n",
      "[0.088135   0.82791567 0.8801433  0.32333207]\n",
      "[-0.56913304  0.7752509  -0.59975106  0.85904425]\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "\n",
    "environment.reset()\n",
    "done = False\n",
    "for i in range(100):\n",
    "    if not done:\n",
    "        state, reward, terminated, truncated, _  = environment.step(environment.action_space.sample())\n",
    "        print(environment.action_space.sample())\n",
    "        done = terminated or truncated\n",
    "        # print(reward)\n",
    "    else:\n",
    "        break\n",
    "environment.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T04:05:43.895085Z",
     "start_time": "2024-11-20T04:05:41.585571Z"
    }
   },
   "id": "6e3e89d3262630d4",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
